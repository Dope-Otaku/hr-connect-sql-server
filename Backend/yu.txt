1. we need to create a function where it will take param id as parameters

2. the structure will be like this


def param_combi(paramID):
    fetch = connect.database
    output = fetch[-].all
    return output

3. in the above function we will take paramID as arguments and fetch data from table using this id

4. the result should be stored in seperate variable and should be returned

5. create a function which converts the data into cateogary wise

6. create another function which inserts the data but checks the table number



The batch insert with a temporary table reduces the number of network round-trips and leverages SQL Server's ability to efficiently insert data from one table to another.

def batch_insert_with_temp_table(param_ids):
    conn = pyodbc.connect(conn_str)
    cursor = conn.cursor()
    
    try:
        # Create a temporary table
        cursor.execute("""
            CREATE TABLE #TempHRData (
                Id INT,
                ColumnName VARCHAR(10),
                Value VARCHAR(100)
            )
        """)
        
        for param_id in param_ids:
            # Collect data for each param_id
            data = collect_data_for_param(param_id)
            
            # Insert data into temporary table
            cursor.executemany("""
                INSERT INTO #TempHRData (Id, ColumnName, Value)
                VALUES (?, ?, ?)
            """, data)
        
        # Insert from temporary table to actual table
        cursor.execute("""
            INSERT INTO GW_2_P10_HR_2024_old (Id, ColumnName, Value)
            SELECT * FROM #TempHRData
        """)
        
        conn.commit()
    except Exception as e:
        conn.rollback()
        print(f"An error occurred: {e}")
    finally:
        cursor.close()
        conn.close()

def collect_data_for_param(param_id):
    # Your existing logic to collect data for a param_id
    # Return a list of tuples (Id, ColumnName, Value)
    pass



The bulk insert with CSV utilizes SQL Server's BULK INSERT capability, which can be very fast for large datasets.
import csv
import tempfile

def bulk_insert_with_csv(param_ids):
    conn = pyodbc.connect(conn_str)
    cursor = conn.cursor()
    
    try:
        # Create a temporary CSV file
        with tempfile.NamedTemporaryFile(mode='w', delete=False, newline='') as temp_file:
            csv_writer = csv.writer(temp_file)
            
            for param_id in param_ids:
                data = collect_data_for_param(param_id)
                csv_writer.writerows(data)
            
            temp_file_path = temp_file.name
        
        # Bulk insert from CSV
        cursor.execute(f"""
            BULK INSERT GW_2_P10_HR_2024_old
            FROM '{temp_file_path}'
            WITH (
                FIELDTERMINATOR = ',',
                ROWTERMINATOR = '\\n',
                FIRSTROW = 1
            )
        """)
        
        conn.commit()
    except Exception as e:
        conn.rollback()
        print(f"An error occurred: {e}")
    finally:
        cursor.close()
        conn.close()
        os.unlink(temp_file_path)  # Delete the temporary file

def collect_data_for_param(param_id):
    # Your existing logic to collect data for a param_id
    # Return a list of lists [Id, ColumnName, Value]
    pass



def collect_data_for_param(param_id, cursor):
    param_results, object_value_table = param_combi(param_id)
    object_values = get_object_values(param_results, object_value_table)
    
    #c007 c008 c010
    # Initialize variables
    product_count = None
    product_type = None
    downtime = None
    start_time = None

    # Assign values based on their position in the list
    if len(object_values) >= 3:
        product_count = object_values[0]['Value']
        product_type = object_values[1]['Value']
        downtime = object_values[2]['Value']
        start_time = object_values[0]['TimeStamp']

    if start_time is None:
        # If no start_time was found, use current time as fallback
        start_time = datetime.now()


    # print(start_time)
    current_date, start_time_str = start_time.split()

    # Remove the microseconds part if it exists
    start_time_str = start_time_str.split('.')[0]


    # Parse the original time
    original_time_obj = datetime.strptime(start_time_str, "%H:%M:%S")

    # Round down to the nearest hour
    start_time_obj = original_time_obj.replace(minute=0, second=0, microsecond=0)
    start_time_str = start_time_obj.strftime("%H:%M")

    # Set end time to the next hour
    end_time_obj = start_time_obj + timedelta(hours=1)
    end_time_str = end_time_obj.strftime("%H:%M")

    # print(current_date, start_time_str, end_time_str)




    # Get shift data
    shift_name = get_shift_data(cursor, start_time_str)

    cursor.execute("SELECT MAX(Id) FROM GW_2_P10_HR_2024_old")
    max_id = cursor.fetchone()[0]
    new_id = (max_id or 0) + 1

    data = []

    # Get the range for C-codes based on param_id
    start_c, end_c = cal_for_table(param_id)
    start_num = int(start_c[1:])
    end_num = int(end_c[1:])

    for i in range(1, end_num + 1):  # Always start from C001 and go up to the end C-code
        c_code = f'C{i:03d}'
        value = None

        if i == 1:  # C001: Timestamp
            value = f"{current_date} {start_time_str}:00"
        elif i == 2:  # C002: Current Date
            value = current_date
        elif i == 3:  # C003: Current Time
            value = start_time_str
        elif i == 4:  # C004: End Time
            value = end_time_str
        elif i == 5:  # C005: Shift Name
            value = shift_name
        elif i == 6:  # C006: Default to '0'
            value = '0'
        elif i == start_num + 1:  # C007 or equivalent: Downtime
            value = str(downtime) if downtime is not None else None
        elif i == start_num + 2:  # C008 or equivalent: Product Type
            value = str(product_type) if product_type is not None else None
        elif i == start_num + 3:  # C009 or equivalent: Default to '0'
            value = '0'
        elif i == start_num + 4:  # C010 or equivalent: Product Count
            value = str(product_count) if product_count is not None else None
        elif i == start_num + 5:  # C011 or equivalent: Default to 'TPM'
            value = 'TPM'
        elif i == start_num + 6:  # C012 or equivalent: Default to '0'
            value = '0'
        elif i == start_num + 7:  # C013 or equivalent: Same as Downtime
            value = str(downtime) if downtime is not None else None
        
        data.append((new_id, c_code, value))

    print(f"Data collected for param_id {param_id}:")
    for row in data:
        print(f"  {row[1]}: {row[2]}")

    return data    



--------------------------------------------------------

arm gripper exercises

5 exercises for 30 sec each and 10 secs rest in between!





The asynchronous insert with multiprocessing parallelizes the data collection and insertion process, which can be beneficial if you have multiple cores available and the bottleneck is in data processing rather than database insertion.

import multiprocessing
from functools import partial

def async_insert_with_multiprocessing(param_ids):
    pool = multiprocessing.Pool()
    
    try:
        # Use partial to pass the connection string to the worker function
        worker_func = partial(process_param_id, conn_str=conn_str)
        
        # Map param_ids to worker processes
        pool.map(worker_func, param_ids)
    finally:
        pool.close()
        pool.join()

def process_param_id(param_id, conn_str):
    conn = pyodbc.connect(conn_str)
    cursor = conn.cursor()
    
    try:
        data = collect_data_for_param(param_id)
        
        cursor.executemany("""
            INSERT INTO GW_2_P10_HR_2024_old (Id, ColumnName, Value)
            VALUES (?, ?, ?)
        """, data)
        
        conn.commit()
    except Exception as e:
        conn.rollback()
        print(f"An error occurred for param_id {param_id}: {e}")
    finally:
        cursor.close()
        conn.close()

def collect_data_for_param(param_id):
    # Your existing logic to collect data for a param_id
    # Return a list of tuples (Id, ColumnName, Value)
    pass



